{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DTSA 5011 Introduction to Deep Learning Final Project","metadata":{}},{"cell_type":"markdown","source":"![](https://raw.githubusercontent.com/cloud-erik/5510/main/boulder.jpg)","metadata":{}},{"cell_type":"markdown","source":"\n\nManny thanks to Titouan Lorieul. I used his great getting startet notbooks related to this competition\n\n- https://www.kaggle.com/code/tlorieul/geolifeclef2022-data-loading-and-visualization\n- https://www.kaggle.com/code/tlorieul/geolifeclef2022-baselines-and-submission\n\nto get a quick start with the complex structure of the data provided.","metadata":{}},{"cell_type":"markdown","source":"# About the project\n\nI decided to go as final project for a challanging Kagggle competition GeoLifeCLEF 2022 - LifeCLEF 2022 x FGVC9, Location-based species presence prediction https://www.kaggle.com/competitions/geolifeclef-2022-lifeclef-2022-fgvc9.\n\nThe aim of this competition is to predict the localization of plant and animal species.\nTo do so, 1.6M geo-localized observations from France and the US of 17K species are provided (9K plant species and 8K animal species).\nThese observations are paired with aerial images and environmental features around them (as illustrated above).\nThe goal is, for each GPS position in the test set (for which we provide the associated aerial images and environmental features), to return a set of candidate species that should contain the true observed species. \n\nThe competition is part of the CLEF 2022 Conference and Labs of the Evaluation Forum (https://clef2022.clef-initiative.eu/)\n\nThere is also an additional github repository for this competition with relevant helper functions to handle the data: https://github.com/maximiliense/GLC\n\n","metadata":{}},{"cell_type":"markdown","source":"# Data\nThe available data for ntraining consits of 1.6 million of geo-localized observations of plants and animals in the US and France.\n\nThere are 17K species in the dataset, 9K plant species and 8K animal species.\n\nEach observation consists of a species name with the GPS coordinates where it was observed.\n\nAlso for every location there is a\n- RGB satellite image,\n- a near IR satellite image,\n- a map with the altitude,\n- a map with land cover\n- and 19 low-resolution rasters with Bioclimatic data\n- and 8 low-resolution rasters with Pedologic data\n\nprovided. In full we have more as 61 GB of data available for training and testing.\n\n# Approch\n\nBecause this is the final project of the deep learning course I will choose a deep learning approch. I will try to build a CNN that is able to classify the stacked RGB, IR, altitude and land cover images. So Input will be an image with resolution of 256x256 pixels and 6 channels. After the covolution layers I will merge this network with a second deep dense network that takes the Bioclimatic and Pedologic data of the observasion as a kind of matadata also into account. The joined network than should be responsible for teh final prediction.\n\nI am fully aware that this is a complex and also computational challanging project and I hope I will be able to get it at least somehow to an end. And it would be fantastic but illosoric if I could produce a competable submission.","metadata":{}},{"cell_type":"markdown","source":"# Loading and analysing data\nStart with the data structure and  load the data.","metadata":{}},{"cell_type":"code","source":"# for performace patch\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn() # with larger no of trees kernel dies\n \n# disable  patching\n#from sklearnex import unpatch_sklearn\n#unpatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:46:04.057105Z","iopub.execute_input":"2022-05-10T18:46:04.058493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pylab inline --no-import-all\n\nfrom pathlib import Path","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:43.556762Z","start_time":"2022-02-15T14:44:42.730071Z"},"execution":{"iopub.status.busy":"2022-05-10T18:25:58.615981Z","iopub.execute_input":"2022-05-10T18:25:58.618327Z","iopub.status.idle":"2022-05-10T18:25:58.688672Z","shell.execute_reply.started":"2022-05-10T18:25:58.617704Z","shell.execute_reply":"2022-05-10T18:25:58.687412Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"We first need to clone the github repository of the project","metadata":{}},{"cell_type":"code","source":"!rm -rf GLC\n!git clone https://github.com/maximiliense/GLC","metadata":{"execution":{"iopub.status.busy":"2022-05-10T18:25:59.006861Z","iopub.execute_input":"2022-05-10T18:25:59.007454Z","iopub.status.idle":"2022-05-10T18:26:01.891840Z","shell.execute_reply.started":"2022-05-10T18:25:59.007401Z","shell.execute_reply":"2022-05-10T18:26:01.891053Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Then, we need to define the path to the data:","metadata":{}},{"cell_type":"code","source":"# Change this path to adapt to where you downloaded the data\nDATA_PATH = Path(\"../input/geolifeclef-2022-lifeclef-2022-fgvc9/\")","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:43.562397Z","start_time":"2022-02-15T14:44:43.55931Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:01.896039Z","iopub.execute_input":"2022-05-10T18:26:01.896351Z","iopub.status.idle":"2022-05-10T18:26:01.901835Z","shell.execute_reply.started":"2022-05-10T18:26:01.896311Z","shell.execute_reply":"2022-05-10T18:26:01.901183Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"This folder is the path root where the data was downloaded and extracted:","metadata":{}},{"cell_type":"code","source":"ls -L $DATA_PATH","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:43.695547Z","start_time":"2022-02-15T14:44:43.563977Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:01.903395Z","iopub.execute_input":"2022-05-10T18:26:01.904011Z","iopub.status.idle":"2022-05-10T18:26:02.678409Z","shell.execute_reply.started":"2022-05-10T18:26:01.903972Z","shell.execute_reply":"2022-05-10T18:26:02.677479Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"We can now look into these subfolders and the data they contain.","metadata":{}},{"cell_type":"markdown","source":"# Observations\n\nThe `observations` subfolder contains 4 CSV files:","metadata":{}},{"cell_type":"code","source":"ls $DATA_PATH/observations","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:43.842493Z","start_time":"2022-02-15T14:44:43.705235Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:02.681296Z","iopub.execute_input":"2022-05-10T18:26:02.682542Z","iopub.status.idle":"2022-05-10T18:26:03.436187Z","shell.execute_reply.started":"2022-05-10T18:26:02.682489Z","shell.execute_reply":"2022-05-10T18:26:03.435323Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Each of line of those files corresponds to a single observation.\n\nIn the files corresponding to the training data, there are 5 columns:\n- `observation_id`: unique identifier of the observation\n- `latitude`: latitude coordinates of this observation\n- `longitude`: longitude coordinates of this observation\n- `species_id`: identifier of the species observed at that location\n- `subset`: proposed train/val split using the same splitting procedure than for train and test (equal to either \"train\" or \"val\")\n\nIn the files corresponding to the test data, there are only 3 columns:\n- `observation_id`: unique identifier of the observation\n- `latitude`: latitude coordinates of this observation\n- `longitude`: longitude coordinates of this observation\n\nThe goal is then to predict the identifier of the species observed at that location.","metadata":{"ExecuteTime":{"end_time":"2021-03-05T18:10:04.278128Z","start_time":"2021-03-05T18:10:04.273719Z"}}},{"cell_type":"markdown","source":"Let's load these CSV files using [pandas](https://pandas.pydata.org/):","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:44.127475Z","start_time":"2022-02-15T14:44:43.849489Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:03.438703Z","iopub.execute_input":"2022-05-10T18:26:03.439781Z","iopub.status.idle":"2022-05-10T18:26:03.445091Z","shell.execute_reply.started":"2022-05-10T18:26:03.439731Z","shell.execute_reply":"2022-05-10T18:26:03.444486Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\ndf_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n\ndf_obs = pd.concat((df_obs_fr, df_obs_us))\n\nprint(\"Number of observations for training: {}\".format(len(df_obs)))\n\ndf_obs.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:45.101516Z","start_time":"2022-02-15T14:44:44.129115Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:03.446732Z","iopub.execute_input":"2022-05-10T18:26:03.447648Z","iopub.status.idle":"2022-05-10T18:26:05.708612Z","shell.execute_reply.started":"2022-05-10T18:26:03.447604Z","shell.execute_reply":"2022-05-10T18:26:05.707767Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\ndf_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n\ndf_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n\nprint(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n\ndf_obs_test.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:45.132658Z","start_time":"2022-02-15T14:44:45.103356Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:05.710899Z","iopub.execute_input":"2022-05-10T18:26:05.711643Z","iopub.status.idle":"2022-05-10T18:26:05.777908Z","shell.execute_reply.started":"2022-05-10T18:26:05.711603Z","shell.execute_reply":"2022-05-10T18:26:05.776937Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The observations are not uniformly sampled in the two countries as shown the following plots.\nThe training observations are shown in blue while the test ones are shown in red.","metadata":{}},{"cell_type":"code","source":"from GLC.plotting import plot_map\n\n\ndef plot_observations_distribution(ax, df_obs, df_obs_test=None, **kwargs):\n    default_kwargs = {\n        \"zorder\": 1,\n        \"alpha\": 0.1,\n        \"s\": 0.5\n    }\n    default_kwargs.update(kwargs)\n    kwargs = default_kwargs\n    \n    ax.scatter(df_obs.longitude, df_obs.latitude, color=\"blue\", **kwargs)\n    \n    if df_obs_test is not None:\n        ax.scatter(df_obs_test.longitude, df_obs_test.latitude, color=\"red\", **kwargs)\n\n\nfig = plt.figure(figsize=(10, 5.5))\nax = plot_map(region=\"us\")\nplot_observations_distribution(ax, df_obs_us, df_obs_us_test)\nax.set_title(\"Observations distribution (US)\")\n\nfig = plt.figure(figsize=(8, 8))\nax = plot_map(region=\"fr\")\nplot_observations_distribution(ax, df_obs_fr, df_obs_fr_test)\nax.set_title(\"Observations distribution (France)\")","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:56.860052Z","start_time":"2022-02-15T14:44:45.134254Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:05.779099Z","iopub.execute_input":"2022-05-10T18:26:05.779339Z","iopub.status.idle":"2022-05-10T18:26:24.117425Z","shell.execute_reply.started":"2022-05-10T18:26:05.779313Z","shell.execute_reply":"2022-05-10T18:26:24.116538Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"A close-up view on the region around Montpellier, France, shows the train/test splitting procedure.\n\nNote that there is no geographical overlap between training and test sets.","metadata":{}},{"cell_type":"code","source":"def select_samples_around_point(df_obs, lon_min, lon_max, lat_min, lat_max):\n    ind = (\n        (lon_min <= df_obs.longitude) & (df_obs.longitude <= lon_max)\n        & (lat_min <= df_obs.latitude) & (df_obs.latitude <= lat_max)\n    )\n    return df_obs[ind]\n\n\nextent = [3, 4.5, 43.25, 44.25]\n\nfig = plt.figure(figsize=(9.5, 7))\nax = plot_map(extent=extent)\n\ndf_obs_zoom = select_samples_around_point(df_obs_fr, *extent)\ndf_obs_zoom_test = select_samples_around_point(df_obs_fr_test, *extent)\n\nkwargs = {\n    \"alpha\": 0.2,\n    \"s\": 5,\n}\nplot_observations_distribution(ax, df_obs_zoom, df_obs_zoom_test, **kwargs)\nax.set_title(\"Observations distribution around Montpellier, France\")","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:57.740705Z","start_time":"2022-02-15T14:44:56.861597Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:24.119167Z","iopub.execute_input":"2022-05-10T18:26:24.119687Z","iopub.status.idle":"2022-05-10T18:26:24.982963Z","shell.execute_reply.started":"2022-05-10T18:26:24.119653Z","shell.execute_reply":"2022-05-10T18:26:24.982173Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The dataset contains 17K species and is imbalanced.","metadata":{}},{"cell_type":"code","source":"species_value_counts = df_obs[\"species_id\"].value_counts()\n\nprint(\"Total number of species: {}\".format(len(species_value_counts)))\n\n\nfig = plt.figure()\nax = fig.gca()\n\nx = np.arange(len(species_value_counts))\nax.plot(x, species_value_counts)\n\nax.set_yscale(\"log\")\n\nax.set_xlabel(\"ranked species\")\nax.set_ylabel(\"number of observations per species\")\nax.set_title(\"Species observations distribution\")\n\nax.grid()\nax.autoscale(tight=True)\nax.set_ylim(bottom=1)","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:58.590738Z","start_time":"2022-02-15T14:44:57.74312Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:25.728453Z","iopub.execute_input":"2022-05-10T18:26:25.728683Z","iopub.status.idle":"2022-05-10T18:26:26.271751Z","shell.execute_reply.started":"2022-05-10T18:26:25.728656Z","shell.execute_reply":"2022-05-10T18:26:26.271062Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Metadata\n\nIn the `metadata` folder, some additional data is provided.\nThere are 4 files containing:\n1. GBIF species, genus, families and kingdom names associated with the species id provided in the observations in `species_details.csv`\n2. The description of the environmental (bioclimatic and pedological) variables in `environmental_variables.csv`\n3. The labels corresponding to the original land cover codes in `landcover_original_labels.csv`\n4. The suggested alignment of land cover codes between France and US in `landcover_suggested_alignment.csv`","metadata":{}},{"cell_type":"code","source":"df_species = pd.read_csv(DATA_PATH / \"metadata\" / \"species_details.csv\", sep=\";\")\n\nprint(\"Total number of species: {}\".format(len(df_species)))\n\nprint(\"\\nNumber of species in each kingdom:\")\nprint(df_species.GBIF_kingdom_name.value_counts())\n\ndf_species.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:58.624516Z","start_time":"2022-02-15T14:44:58.59234Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:26.272765Z","iopub.execute_input":"2022-05-10T18:26:26.273330Z","iopub.status.idle":"2022-05-10T18:26:26.334888Z","shell.execute_reply.started":"2022-05-10T18:26:26.273294Z","shell.execute_reply":"2022-05-10T18:26:26.334058Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_obs = df_obs.reset_index().merge(df_species, on=\"species_id\", how=\"left\").set_index(df_obs.index.names)\n\nprint(\"Number of observations of each kingdom:\")\nprint(df_obs.GBIF_kingdom_name.value_counts())\n\ndf_obs.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.194041Z","start_time":"2022-02-15T14:44:58.626304Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:26.337452Z","iopub.execute_input":"2022-05-10T18:26:26.337839Z","iopub.status.idle":"2022-05-10T18:26:27.425303Z","shell.execute_reply.started":"2022-05-10T18:26:26.337793Z","shell.execute_reply":"2022-05-10T18:26:27.424387Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df_env_vars = pd.read_csv(DATA_PATH / \"metadata\" / \"environmental_variables.csv\", sep=\";\")\ndf_env_vars.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.205241Z","start_time":"2022-02-15T14:44:59.196241Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:27.426838Z","iopub.execute_input":"2022-05-10T18:26:27.427160Z","iopub.status.idle":"2022-05-10T18:26:27.449424Z","shell.execute_reply.started":"2022-05-10T18:26:27.427118Z","shell.execute_reply":"2022-05-10T18:26:27.448448Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df_landcover_labels = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_original_labels.csv\", sep=\";\")\ndf_landcover_labels.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.218883Z","start_time":"2022-02-15T14:44:59.20665Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:27.450848Z","iopub.execute_input":"2022-05-10T18:26:27.451813Z","iopub.status.idle":"2022-05-10T18:26:27.472183Z","shell.execute_reply.started":"2022-05-10T18:26:27.451765Z","shell.execute_reply":"2022-05-10T18:26:27.471038Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_suggested_landcover_alignment = pd.read_csv(DATA_PATH / \"metadata\" / \"landcover_suggested_alignment.csv\", sep=\";\")\ndf_suggested_landcover_alignment.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.2324Z","start_time":"2022-02-15T14:44:59.220466Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:27.473992Z","iopub.execute_input":"2022-05-10T18:26:27.474349Z","iopub.status.idle":"2022-05-10T18:26:27.491863Z","shell.execute_reply.started":"2022-05-10T18:26:27.474303Z","shell.execute_reply":"2022-05-10T18:26:27.490874Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Patches\n\nThe patches consist of images centered at each observation's location capturing three types of information in the 250m x 250m neighboring square:\n1. remote sensing imagery under the form of RGB-IR images\n2. land cover data\n3. altitude data\n\nThey are located in the two subfolder `patches-fr` and `patches-us`, one for each country:","metadata":{}},{"cell_type":"code","source":"ls $DATA_PATH","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.374884Z","start_time":"2022-02-15T14:44:59.239101Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:30.460963Z","iopub.execute_input":"2022-05-10T18:26:30.461282Z","iopub.status.idle":"2022-05-10T18:26:31.229883Z","shell.execute_reply.started":"2022-05-10T18:26:30.461220Z","shell.execute_reply":"2022-05-10T18:26:31.228668Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The first digit of the observation id tells the country it belongs to:\n- `1` for France, thus to be found in subfolder `patches-fr`\n- `2` for US, thus to be found in subfolder `patches-us`\n\nFor instance, `10561900` is an observation made in France (on the Pic Saint-Loup mountain) whereas `22068175` was observed in the US.\n\nInside those folders, there are two levels of hierarchy, corresponding to the last four digits of the observation id:","metadata":{}},{"cell_type":"code","source":"ls $DATA_PATH/patches-fr","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.56083Z","start_time":"2022-02-15T14:44:59.381974Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:31.510465Z","iopub.execute_input":"2022-05-10T18:26:31.511010Z","iopub.status.idle":"2022-05-10T18:26:32.306065Z","shell.execute_reply.started":"2022-05-10T18:26:31.510959Z","shell.execute_reply":"2022-05-10T18:26:32.304651Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"and","metadata":{}},{"cell_type":"code","source":"ls $DATA_PATH/patches-fr/00","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.716737Z","start_time":"2022-02-15T14:44:59.56795Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:32.403731Z","iopub.execute_input":"2022-05-10T18:26:32.404061Z","iopub.status.idle":"2022-05-10T18:26:33.185615Z","shell.execute_reply.started":"2022-05-10T18:26:32.404024Z","shell.execute_reply":"2022-05-10T18:26:33.184356Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"To find the files corresponding to an observation:\n1. the first subfolder corresponds to the last two digits,\n2. the second subfolder corresponds to the two digits right before them.\n\nFor instance, the patches corresponding to observation `10171444` can be found in `patches-fr/44/14`, whereas `22068100` can be found in `patches-us/00/81`:","metadata":{}},{"cell_type":"code","source":"ls $DATA_PATH/patches-fr/44/14/10171444*","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:44:59.85395Z","start_time":"2022-02-15T14:44:59.719592Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:33.555361Z","iopub.execute_input":"2022-05-10T18:26:33.556209Z","iopub.status.idle":"2022-05-10T18:26:34.416862Z","shell.execute_reply.started":"2022-05-10T18:26:33.556163Z","shell.execute_reply":"2022-05-10T18:26:34.415919Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"and","metadata":{}},{"cell_type":"code","source":"ls $DATA_PATH/patches-us/00/81/22068100*","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:00.016062Z","start_time":"2022-02-15T14:44:59.859568Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:34.418799Z","iopub.execute_input":"2022-05-10T18:26:34.419067Z","iopub.status.idle":"2022-05-10T18:26:35.334691Z","shell.execute_reply.started":"2022-05-10T18:26:34.419037Z","shell.execute_reply":"2022-05-10T18:26:35.333778Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"There are 4 files for each observation:\n- a color JPEG image containing an RGB image (`*_rgb.jpg`)\n- a grayscale JPEG image containing a near-infrared image (`*_near_ir.jpg`)\n- a TIFF with Deflate compression containing altitude data (`*_altitude.tif`)\n- a TIFF with Deflate compression containing land cover data (`*_landcover.tif`)\n\nWe provide a loading function which, given an observation id, loads all this data at once using [Pillow](https://pillow.readthedocs.io/en/stable/) for the images and [tiffile](https://github.com/cgohlke/tifffile) for the TIFF files and returns them as a tuple `(rgb, near-ir, altitude, landcover)`:","metadata":{}},{"cell_type":"code","source":"from GLC.data_loading.common import load_patch\n\npatch = load_patch(10171444, DATA_PATH)\n\nprint(\"Number of data sources: {}\".format(len(patch)))\nprint(\"Arrays shape: {}\".format([p.shape for p in patch]))\nprint(\"Data types: {}\".format([p.dtype for p in patch]))","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:00.07101Z","start_time":"2022-02-15T14:45:00.021859Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:35.336917Z","iopub.execute_input":"2022-05-10T18:26:35.337748Z","iopub.status.idle":"2022-05-10T18:26:35.544390Z","shell.execute_reply.started":"2022-05-10T18:26:35.337699Z","shell.execute_reply":"2022-05-10T18:26:35.543200Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"It can also automatically perform the land cover alignment if necessary:","metadata":{}},{"cell_type":"code","source":"landcover_mapping = df_suggested_landcover_alignment[\"suggested_landcover_code\"].values\npatch = load_patch(10171444, DATA_PATH, landcover_mapping=landcover_mapping)","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:00.082686Z","start_time":"2022-02-15T14:45:00.072933Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:36.360087Z","iopub.execute_input":"2022-05-10T18:26:36.360863Z","iopub.status.idle":"2022-05-10T18:26:36.378821Z","shell.execute_reply.started":"2022-05-10T18:26:36.360806Z","shell.execute_reply":"2022-05-10T18:26:36.378189Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"We also provide an visualization function for the patches:","metadata":{}},{"cell_type":"code","source":"from GLC.plotting import visualize_observation_patch\n\n# Extracts land cover labels\nlandcover_labels = df_suggested_landcover_alignment[[\"suggested_landcover_code\", \"suggested_landcover_label\"]].drop_duplicates().sort_values(\"suggested_landcover_code\")[\"suggested_landcover_label\"].values\n\nvisualize_observation_patch(patch, observation_data=df_obs.loc[10561900], landcover_labels=landcover_labels)","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:02.267074Z","start_time":"2022-02-15T14:45:00.084242Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:37.335207Z","iopub.execute_input":"2022-05-10T18:26:37.336003Z","iopub.status.idle":"2022-05-10T18:26:39.791825Z","shell.execute_reply.started":"2022-05-10T18:26:37.335960Z","shell.execute_reply":"2022-05-10T18:26:39.791164Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Similarly, for the observation `22068100`:","metadata":{}},{"cell_type":"code","source":"patch = load_patch(22068100, DATA_PATH, landcover_mapping=landcover_mapping)\n\nvisualize_observation_patch(patch, observation_data=df_obs.loc[22068100], landcover_labels=landcover_labels)","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:03.259998Z","start_time":"2022-02-15T14:45:02.269008Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:39.793098Z","iopub.execute_input":"2022-05-10T18:26:39.793417Z","iopub.status.idle":"2022-05-10T18:26:41.104414Z","shell.execute_reply.started":"2022-05-10T18:26:39.793389Z","shell.execute_reply":"2022-05-10T18:26:41.103634Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Environmental rasters\n\nThe rasters contain low-resolution environmental data - bioclimatic and pedological data.\n\nThere are two ways to use this data:\n1. directly use the environmental vectors pre-extracted that can be found in the CSV file `pre-extracted/environmental_vectors.csv`\n2. manually extract patches centered at each observation using the rasters located in the `rasters` subfolder","metadata":{}},{"cell_type":"markdown","source":"## Pre-extracted environmental vectors\n\nThese vectors are ready to be used. They are easy to load as they are provided as a CSV file.\n\nEach line of this file correspond to an observation and each column to one of the environmental variable.","metadata":{}},{"cell_type":"code","source":"df_env = pd.read_csv(DATA_PATH / \"pre-extracted\" / \"environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")\ndf_env.head()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:10.974172Z","start_time":"2022-02-15T14:45:03.262207Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:44.304713Z","iopub.execute_input":"2022-05-10T18:26:44.305156Z","iopub.status.idle":"2022-05-10T18:26:56.485095Z","shell.execute_reply.started":"2022-05-10T18:26:44.305122Z","shell.execute_reply":"2022-05-10T18:26:56.484188Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Note that it typically contains NaN values due to absence of data over the seas and oceans for both types of data as well as rivers and others for the pedologic data.","metadata":{"ExecuteTime":{"end_time":"2021-03-05T19:46:04.789976Z","start_time":"2021-03-05T19:46:04.784343Z"}}},{"cell_type":"code","source":"print(\"Variables which can contain NaN values:\")\ndf_env.isna().any()","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:11.010466Z","start_time":"2022-02-15T14:45:10.975663Z"},"execution":{"iopub.status.busy":"2022-05-10T18:26:56.486851Z","iopub.execute_input":"2022-05-10T18:26:56.487101Z","iopub.status.idle":"2022-05-10T18:26:56.536779Z","shell.execute_reply.started":"2022-05-10T18:26:56.487070Z","shell.execute_reply":"2022-05-10T18:26:56.535830Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Patch extraction from rasters\n\nTo more easily extract patches from the rasters, we provide a `PatchExtractor` class which uses [rasterio](https://github.com/mapbox/rasterio).","metadata":{}},{"cell_type":"code","source":"from GLC.data_loading.environmental_raster import PatchExtractor","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:45:11.147666Z","start_time":"2022-02-15T14:45:11.015778Z"},"execution":{"iopub.status.busy":"2022-05-10T10:04:50.775346Z","iopub.execute_input":"2022-05-10T10:04:50.776134Z","iopub.status.idle":"2022-05-10T10:04:51.418276Z","shell.execute_reply.started":"2022-05-10T10:04:50.776083Z","shell.execute_reply":"2022-05-10T10:04:51.41736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following code loads the rasters for all the variables and prepares to extract patches of size 256x256.\n\nHere the patches are not of the same resolution as the provided ones as one pixel corresponds to 30arcsec (~1km) for the bioclimatic data and to 250m for the pedologic data.\n\nNote that this uses quite a lot of memory (~18Go) as all the rasters will be loaded in the RAM.\n\nTo avoid this issue, we will only load the bioclimatic rasters here.","metadata":{}},{"cell_type":"code","source":"extractor = PatchExtractor(DATA_PATH / \"rasters\", size=256)\nextractor.add_all_bioclimatic_rasters()\n\nprint(\"Number of rasters: {}\".format(len(extractor)))","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:46:21.45614Z","start_time":"2022-02-15T14:45:11.149698Z"},"execution":{"iopub.status.busy":"2022-05-10T10:04:53.248096Z","iopub.execute_input":"2022-05-10T10:04:53.248424Z","iopub.status.idle":"2022-05-10T10:05:16.682365Z","shell.execute_reply.started":"2022-05-10T10:04:53.248387Z","shell.execute_reply":"2022-05-10T10:05:16.681503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To load all the rasters use:\n```\nextractor.add_all_rasters()\n```\nTo load all the pedologic rasters use:\n```\nextractor.add_all_pedologic_rasters()`\n```","metadata":{}},{"cell_type":"markdown","source":"A patch can then easily to be extracted given the localization using:","metadata":{}},{"cell_type":"code","source":"patch = extractor[43.61, 3.88]\n\nprint(\"Patch shape: {}\".format(patch.shape))\nprint(\"Data type: {}\".format(patch.dtype))","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:46:21.464289Z","start_time":"2022-02-15T14:46:21.458017Z"},"execution":{"iopub.status.busy":"2022-05-10T10:05:30.465221Z","iopub.execute_input":"2022-05-10T10:05:30.46551Z","iopub.status.idle":"2022-05-10T10:05:30.473497Z","shell.execute_reply.started":"2022-05-10T10:05:30.465479Z","shell.execute_reply":"2022-05-10T10:05:30.472525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that it typically contains NaN values due to absence of data over the seas and oceans for both types of data as well as rivers and others for the pedologic data.","metadata":{}},{"cell_type":"code","source":"print(\"Contains NaN: {}\".format(np.isnan(patch).any()))","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:46:21.487561Z","start_time":"2022-02-15T14:46:21.465832Z"},"execution":{"iopub.status.busy":"2022-05-10T10:05:33.062118Z","iopub.execute_input":"2022-05-10T10:05:33.062551Z","iopub.status.idle":"2022-05-10T10:05:33.069265Z","shell.execute_reply.started":"2022-05-10T10:05:33.062506Z","shell.execute_reply":"2022-05-10T10:05:33.068286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A helper function to plot the patches is also provided.\n\nThe following example displays the patches obtained around the region of Montpellier, France.","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 10))\nextractor.plot((43.61, 3.88), fig=fig)","metadata":{"ExecuteTime":{"end_time":"2022-02-15T14:46:25.888183Z","start_time":"2022-02-15T14:46:21.491586Z"},"execution":{"iopub.status.busy":"2022-05-10T10:05:37.051806Z","iopub.execute_input":"2022-05-10T10:05:37.052102Z","iopub.status.idle":"2022-05-10T10:05:41.150509Z","shell.execute_reply.started":"2022-05-10T10:05:37.052071Z","shell.execute_reply":"2022-05-10T10:05:41.149782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"import os\n\n# Create the path to save submission files\nSUBMISSION_PATH = Path(\"submissions\")\nos.makedirs(SUBMISSION_PATH, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:47:57.852082Z","iopub.execute_input":"2022-05-10T10:47:57.852458Z","iopub.status.idle":"2022-05-10T10:47:57.858388Z","shell.execute_reply.started":"2022-05-10T10:47:57.852424Z","shell.execute_reply":"2022-05-10T10:47:57.857157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also load the official metric, top-30 error rate, for which we provide efficient implementations:\n","metadata":{}},{"cell_type":"code","source":"from GLC.metrics import top_30_error_rate\nhelp(top_30_error_rate)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:49:31.748188Z","iopub.execute_input":"2022-05-10T10:49:31.748732Z","iopub.status.idle":"2022-05-10T10:49:31.753145Z","shell.execute_reply.started":"2022-05-10T10:49:31.748678Z","shell.execute_reply":"2022-05-10T10:49:31.752576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from GLC.metrics import top_k_error_rate_from_sets\nhelp(top_k_error_rate_from_sets)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:49:32.60755Z","iopub.execute_input":"2022-05-10T10:49:32.608038Z","iopub.status.idle":"2022-05-10T10:49:32.612605Z","shell.execute_reply.started":"2022-05-10T10:49:32.607985Z","shell.execute_reply":"2022-05-10T10:49:32.611952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For submissions, we will also need to predict the top-30 sets for which we also provide an efficient implementation:","metadata":{}},{"cell_type":"code","source":"from GLC.metrics import predict_top_30_set\nhelp(predict_top_30_set)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:50:03.23174Z","iopub.execute_input":"2022-05-10T10:50:03.232045Z","iopub.status.idle":"2022-05-10T10:50:03.238202Z","shell.execute_reply.started":"2022-05-10T10:50:03.232011Z","shell.execute_reply":"2022-05-10T10:50:03.236961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also provide an utility function to generate submission files in the right format:","metadata":{}},{"cell_type":"code","source":"from GLC.submission import generate_submission_file\nhelp(generate_submission_file)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:50:21.828984Z","iopub.execute_input":"2022-05-10T10:50:21.829783Z","iopub.status.idle":"2022-05-10T10:50:21.836144Z","shell.execute_reply.started":"2022-05-10T10:50:21.829731Z","shell.execute_reply":"2022-05-10T10:50:21.835429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Observation data loading","metadata":{}},{"cell_type":"markdown","source":"We first need to load the observation data:","metadata":{}},{"cell_type":"code","source":"df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\ndf_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\ndf_obs = pd.concat((df_obs_fr, df_obs_us))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:50:54.83142Z","iopub.execute_input":"2022-05-10T10:50:54.831866Z","iopub.status.idle":"2022-05-10T10:50:56.234417Z","shell.execute_reply.started":"2022-05-10T10:50:54.831834Z","shell.execute_reply":"2022-05-10T10:50:56.233665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we retrieve the train/val split provided:","metadata":{}},{"cell_type":"code","source":"obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\nobs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n\ny_train = df_obs.loc[obs_id_train][\"species_id\"].values\ny_val = df_obs.loc[obs_id_val][\"species_id\"].values\n\nn_val = len(obs_id_val)\nprint(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:51:12.421458Z","iopub.execute_input":"2022-05-10T10:51:12.422111Z","iopub.status.idle":"2022-05-10T10:51:13.195056Z","shell.execute_reply.started":"2022-05-10T10:51:12.422069Z","shell.execute_reply":"2022-05-10T10:51:13.193848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also load the observation data for the test set:","metadata":{}},{"cell_type":"code","source":"df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\ndf_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n\ndf_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n\nobs_id_test = df_obs_test.index.values\n\nprint(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n\ndf_obs_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:51:39.783126Z","iopub.execute_input":"2022-05-10T10:51:39.783466Z","iopub.status.idle":"2022-05-10T10:51:39.837254Z","shell.execute_reply.started":"2022-05-10T10:51:39.783435Z","shell.execute_reply":"2022-05-10T10:51:39.836051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample submission file\n\nIn this section, we will demonstrate how to generate the sample submission file provided.\n\nTo do so, we will use the function `generate_submission_file` from `GLC.submission`.","metadata":{}},{"cell_type":"markdown","source":"The sample submission consists in always predicting the first 30 species for all the test observations:","metadata":{}},{"cell_type":"code","source":"first_30_species = np.arange(30)\ns_pred = np.tile(first_30_species[None], (len(df_obs_test), 1))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:52:46.257275Z","iopub.execute_input":"2022-05-10T10:52:46.257642Z","iopub.status.idle":"2022-05-10T10:52:46.263938Z","shell.execute_reply.started":"2022-05-10T10:52:46.257603Z","shell.execute_reply":"2022-05-10T10:52:46.262738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then generate the associated submission file using:","metadata":{}},{"cell_type":"code","source":"generate_submission_file(SUBMISSION_PATH / \"sample_submission.csv\", df_obs_test.index, s_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:53:06.626359Z","iopub.execute_input":"2022-05-10T10:53:06.627035Z","iopub.status.idle":"2022-05-10T10:53:07.816148Z","shell.execute_reply.started":"2022-05-10T10:53:06.626993Z","shell.execute_reply":"2022-05-10T10:53:07.815091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Constant baseline: 30 most observed species\n\nThe first baseline consists in predicting the 30 most observed species on the train set which corresponds exactly to the \"Top-30 most present species\":","metadata":{}},{"cell_type":"code","source":"species_distribution = df_obs.loc[obs_id_train][\"species_id\"].value_counts(normalize=True)\ntop_30_most_observed = species_distribution.index.values[:30]","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:53:44.368674Z","iopub.execute_input":"2022-05-10T10:53:44.368952Z","iopub.status.idle":"2022-05-10T10:53:44.515872Z","shell.execute_reply.started":"2022-05-10T10:53:44.368923Z","shell.execute_reply":"2022-05-10T10:53:44.514822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, it does not perform very well on the validation set:","metadata":{}},{"cell_type":"code","source":"s_pred = np.tile(top_30_most_observed[None], (n_val, 1))\nscore = top_k_error_rate_from_sets(y_val, s_pred)\nprint(\"Top-30 error rate: {:.1%}\".format(score))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:54:00.301748Z","iopub.execute_input":"2022-05-10T10:54:00.302031Z","iopub.status.idle":"2022-05-10T10:54:00.315515Z","shell.execute_reply.started":"2022-05-10T10:54:00.302001Z","shell.execute_reply":"2022-05-10T10:54:00.314588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will however generate the associated submission file on the test using:","metadata":{}},{"cell_type":"code","source":"# Compute baseline on the test set\nn_test = len(df_obs_test)\ns_pred = np.tile(top_30_most_observed[None], (n_test, 1))\n\n# Generate the submission file\ngenerate_submission_file(SUBMISSION_PATH / \"constant_top_30_most_present_species_baseline.csv\", df_obs_test.index, s_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:54:30.746388Z","iopub.execute_input":"2022-05-10T10:54:30.747083Z","iopub.status.idle":"2022-05-10T10:54:32.032375Z","shell.execute_reply.started":"2022-05-10T10:54:30.747042Z","shell.execute_reply":"2022-05-10T10:54:32.031248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest on environmental vectors\n\nA classical approach in ecology is to train Random Forests on environmental vectors.\n\nWe show here how to do so using [scikit-learn](https://scikit-learn.org/).\n\nWe start by loading the environmental vectors:","metadata":{}},{"cell_type":"code","source":"#df_env = pd.read_csv(DATA_PATH / \"pre-extracted\" / \"environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")\n\nX_train = df_env.loc[obs_id_train].values\nX_val = df_env.loc[obs_id_val].values\nX_test = df_env.loc[obs_id_test].values","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:56:14.872922Z","iopub.execute_input":"2022-05-10T10:56:14.873724Z","iopub.status.idle":"2022-05-10T10:56:25.363366Z","shell.execute_reply.started":"2022-05-10T10:56:14.87367Z","shell.execute_reply":"2022-05-10T10:56:25.362201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we need to handle properly the missing values.\n\nFor instance, using `SimpleImputer`:","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(\n    missing_values=np.nan,\n    strategy=\"constant\",\n    fill_value=np.finfo(np.float32).min,\n)\nimp.fit(X_train)\n\nX_train = imp.transform(X_train)\nX_val = imp.transform(X_val)\nX_test = imp.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:56:45.250208Z","iopub.execute_input":"2022-05-10T10:56:45.250532Z","iopub.status.idle":"2022-05-10T10:56:46.896854Z","shell.execute_reply.started":"2022-05-10T10:56:45.250496Z","shell.execute_reply":"2022-05-10T10:56:46.895595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now start training our Random Forest (as there are a lot of observations, over 1.8M, this can take a while):","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nest = RandomForestClassifier(n_estimators=500, max_depth=15, n_jobs=-1)\nest.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:57:05.570211Z","iopub.execute_input":"2022-05-10T10:57:05.570577Z","iopub.status.idle":"2022-05-10T11:03:55.664354Z","shell.execute_reply.started":"2022-05-10T10:57:05.57054Z","shell.execute_reply":"2022-05-10T11:03:55.663394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As there are a lot of classes (over 17K), we need to be cautious when predicting the scores of the model.\n\nThis can easily take more than 5Go on the validation set.\n\nFor this reason, we will be predict the top-30 sets by batches using the following generic function:","metadata":{}},{"cell_type":"code","source":"def batch_predict(predict_func, X, batch_size=1024):\n    res = predict_func(X[:1])\n    n_samples, n_outputs, dtype = X.shape[0], res.shape[1], res.dtype\n    \n    preds = np.empty((n_samples, n_outputs), dtype=dtype)\n    \n    for i in range(0, len(X), batch_size):\n        X_batch = X[i:i+batch_size]\n        preds[i:i+batch_size] = predict_func(X_batch)\n            \n    return preds","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:03:55.666338Z","iopub.execute_input":"2022-05-10T11:03:55.667006Z","iopub.status.idle":"2022-05-10T11:03:55.676237Z","shell.execute_reply.started":"2022-05-10T11:03:55.666966Z","shell.execute_reply":"2022-05-10T11:03:55.675603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can know compute the top-30 error rate on the validation set:","metadata":{}},{"cell_type":"code","source":"def predict_func(X):\n    y_score = est.predict_proba(X)\n    s_pred = predict_top_30_set(y_score)\n    return s_pred\n\n# Out of Memory with batch size = 2048 and 4096\ns_val = batch_predict(predict_func, X_val, batch_size=1024)\nscore_val = top_k_error_rate_from_sets(y_val, s_val)\nprint(\"Top-30 error rate: {:.1%}\".format(score_val))","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:03:55.677205Z","iopub.execute_input":"2022-05-10T11:03:55.677823Z","iopub.status.idle":"2022-05-10T11:04:36.413607Z","shell.execute_reply.started":"2022-05-10T11:03:55.677783Z","shell.execute_reply":"2022-05-10T11:04:36.412552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now predict the top-30 sets on the test data and save them in a submission file:","metadata":{}},{"cell_type":"code","source":"# Compute baseline on the test set\ns_pred = batch_predict(predict_func, X_test, batch_size=1024)\n\n# Generate the submission file\ngenerate_submission_file(SUBMISSION_PATH / \"random_forest_on_environmental_vectors.csv\", df_obs_test.index, s_pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:04:36.415929Z","iopub.execute_input":"2022-05-10T11:04:36.416533Z","iopub.status.idle":"2022-05-10T11:05:14.555684Z","shell.execute_reply.started":"2022-05-10T11:04:36.416486Z","shell.execute_reply":"2022-05-10T11:05:14.554609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build CNN","metadata":{}},{"cell_type":"code","source":"SUBMISSION_PATH","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:22:00.135502Z","iopub.execute_input":"2022-05-10T11:22:00.135863Z","iopub.status.idle":"2022-05-10T11:22:00.142209Z","shell.execute_reply.started":"2022-05-10T11:22:00.135818Z","shell.execute_reply":"2022-05-10T11:22:00.141196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls -L $SUBMISSION_PATH","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:27:23.006511Z","iopub.execute_input":"2022-05-10T11:27:23.007069Z","iopub.status.idle":"2022-05-10T11:27:23.903427Z","shell.execute_reply.started":"2022-05-10T11:27:23.007008Z","shell.execute_reply":"2022-05-10T11:27:23.902162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}